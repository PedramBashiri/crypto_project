{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2\n",
    "import json\n",
    "import gzip\n",
    "import lzma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'author': 'stunt_penguin', 'author_flair_css_class': None, 'author_flair_text': None, 'body': 'Wheelchairs make amazing dollys :D', 'can_gild': True, 'collapsed': False, 'collapsed_reason': None, 'controversiality': 0, 'created_utc': 1501545600, 'distinguished': None, 'edited': False, 'gilded': 0, 'id': 'dkznc8h', 'is_submitter': False, 'link_id': 't3_6qp8sw', 'parent_id': 't1_dkzbnn3', 'retrieved_on': 1503654247, 'score': 1, 'stickied': False, 'subreddit': 'Filmmakers', 'subreddit_id': 't5_2r1ip'}\n",
      "{'author': '[deleted]', 'author_flair_css_class': None, 'author_flair_text': None, 'body': '[removed]', 'can_gild': True, 'collapsed': False, 'collapsed_reason': None, 'controversiality': 0, 'created_utc': 1501545600, 'distinguished': None, 'edited': False, 'gilded': 0, 'id': 'dkznc8i', 'is_submitter': False, 'link_id': 't3_6qse6i', 'parent_id': 't1_dkzmgc3', 'retrieved_on': 1503654247, 'score': 2, 'stickied': False, 'subreddit': 'Addons4Kodi', 'subreddit_id': 't5_39pp6'}\n",
      "{'author': '69ing', 'author_flair_css_class': None, 'author_flair_text': None, 'body': 'I used to watch the shit out of these guys', 'can_gild': True, 'collapsed': False, 'collapsed_reason': None, 'controversiality': 0, 'created_utc': 1501545600, 'distinguished': None, 'edited': False, 'gilded': 0, 'id': 'dkznc8j', 'is_submitter': False, 'link_id': 't3_6qs8i1', 'parent_id': 't3_6qs8i1', 'retrieved_on': 1503654247, 'score': 5, 'stickied': False, 'subreddit': 'NotTimAndEric', 'subreddit_id': 't5_2tk05'}\n",
      "{'author': 'ArchadianJudge', 'author_flair_css_class': 'Archadianflair', 'author_flair_text': None, 'body': 'http://www.pixiv.net/member_illust.php?mode=medium&amp;illust_id=64137736', 'can_gild': True, 'collapsed': False, 'collapsed_reason': None, 'controversiality': 0, 'created_utc': 1501545600, 'distinguished': None, 'edited': False, 'gilded': 0, 'id': 'dkznc8k', 'is_submitter': True, 'link_id': 't3_6qsicx', 'parent_id': 't3_6qsicx', 'retrieved_on': 1503654247, 'score': 2, 'stickied': False, 'subreddit': 'Saber', 'subreddit_id': 't5_32ud3'}\n",
      "{'author': 'sglville', 'author_flair_css_class': None, 'author_flair_text': None, 'body': \"On the other yand you could say it's capitalism at its finest. If people are buying why not sell it?\", 'can_gild': True, 'collapsed': False, 'collapsed_reason': None, 'controversiality': 0, 'created_utc': 1501545600, 'distinguished': None, 'edited': False, 'gilded': 0, 'id': 'dkznc8l', 'is_submitter': False, 'link_id': 't3_6qryxf', 'parent_id': 't3_6qryxf', 'retrieved_on': 1503654247, 'score': 2, 'stickied': False, 'subreddit': 'The_Donald', 'subreddit_id': 't5_38unr'}\n",
      "{'author': 'NEWORLDODOR', 'author_flair_css_class': None, 'author_flair_text': None, 'body': \"I'm not arguing that making university free and accessible is bad. Thats incredible and done correctly it would be amazing, possibly revolutionary. My problem is that if Peterson successfully pulls off what he's trying to do, then it would be hard for him to avoid the same issues he had with the universities of today and of the last 50 years. His political influence on the subjects would most definitely shape the students of the program, which would be fine if there were alternative education systems with different beliefs, yet in this video it's clear that his plan is to eliminate his educational opposition. \", 'can_gild': True, 'collapsed': False, 'collapsed_reason': None, 'controversiality': 0, 'created_utc': 1501545600, 'distinguished': None, 'edited': False, 'gilded': 0, 'id': 'dkznc8m', 'is_submitter': True, 'link_id': 't3_6qrr1o', 'parent_id': 't1_dkzn053', 'retrieved_on': 1503654247, 'score': 3, 'stickied': False, 'subreddit': 'JordanPeterson', 'subreddit_id': 't5_32jqy'}\n",
      "{'author': 'zachwad22', 'author_flair_css_class': None, 'author_flair_text': None, 'body': \"I like whisper-whistle obsessively, just barely moving enough air to make a sound. I'm sure it drives people insane.\", 'can_gild': True, 'collapsed': False, 'collapsed_reason': None, 'controversiality': 0, 'created_utc': 1501545600, 'distinguished': None, 'edited': False, 'gilded': 0, 'id': 'dkznc8n', 'is_submitter': False, 'link_id': 't3_6qoe6s', 'parent_id': 't3_6qoe6s', 'retrieved_on': 1503654247, 'score': 1, 'stickied': False, 'subreddit': 'AskReddit', 'subreddit_id': 't5_2qh1i'}\n"
     ]
    }
   ],
   "source": [
    "with bz2.open(\"../data/RC_2017-08.bz2\", \"rb\") as f:\n",
    "    for i,line in enumerate(f):\n",
    "        json_obj= json.loads(line)\n",
    "        print(json_obj)\n",
    "        if i > 5:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27414621\n"
     ]
    }
   ],
   "source": [
    "with gzip.open(\"../data/subreddits.json.gz\", \"rb\") as f:\n",
    "    for i,line in enumerate(f):\n",
    "#         json_obj= json.loads(line)\n",
    "#         print(json_obj)\n",
    "#         if i > 2:\n",
    "#             break\n",
    "        pass\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from datetime import date, datetime, timedelta\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting at 2019-10-31 15:54:23.367843\n",
      "Finished at 2019-10-31 16:13:26.376732\n",
      "Total time: --- 1143.0094 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "print(\"Starting at \" + str(datetime.datetime.now()))\n",
    "\n",
    "submissions_dict = dict()\n",
    "\n",
    "with lzma.open(\"../data/RS_2018-03.xz\", \"r\") as f:\n",
    "    for i,line in enumerate(f):\n",
    "        json_obj= json.loads(line)\n",
    "        s_id = json_obj[\"id\"]\n",
    "        \n",
    "        submissions_dict[s_id] = {\n",
    "            'author': json_obj['author'],\n",
    "            'subreddit': json_obj['subreddit'],\n",
    "            'subreddit_id': json_obj['subreddit_id'],\n",
    "            'created_utc': json_obj['created_utc'],\n",
    "            'title': json_obj['title'],\n",
    "            'comment_ids': [],\n",
    "            'score': json_obj['score'],\n",
    "            'selftext': json_obj['selftext']\n",
    "        }\n",
    "            \n",
    "print(\"Finished at \" + str(datetime.datetime.now()))\n",
    "print(\"Total time: --- {0} seconds ---\".format(round(time.time() - start_time, 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting at 2019-10-31 16:13:26.395089\n",
      "Finished at 2019-10-31 17:35:19.663174\n",
      "Total time: --- 4913.2686 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "print(\"Starting at \" + str(datetime.datetime.now()))\n",
    "\n",
    "c_except=0\n",
    "comments_dict = dict()\n",
    "\n",
    "with lzma.open(\"../data/RC_2018-03.xz\", \"r\") as c_f:\n",
    "    for i,line in enumerate(c_f):\n",
    "        json_obj= json.loads(line)\n",
    "        c_id = json_obj[\"id\"]\n",
    "        \n",
    "        link_id_full = json_obj[\"link_id\"]\n",
    "        if link_id_full is None:\n",
    "            link_id = \"N/A\"\n",
    "        else:\n",
    "            link_id = str(link_id_full)[3:]\n",
    "        \n",
    "        parent_id_full = json_obj[\"parent_id\"]\n",
    "        if parent_id_full is None:\n",
    "            parent_id = \"N/A\"\n",
    "        else:\n",
    "            parent_id = str(parent_id_full)[3:]\n",
    "            \n",
    "        comments_dict[c_id] = {\n",
    "            'author': json_obj['author'],\n",
    "            'subreddit': json_obj['subreddit'],\n",
    "            'subreddit_id': json_obj['subreddit_id'],\n",
    "            'created_utc': json_obj['created_utc'],\n",
    "            'parent_id': parent_id,\n",
    "            'link_id': link_id,\n",
    "            'score': json_obj['score'],\n",
    "            'body': json_obj['body']\n",
    "        }\n",
    "        \n",
    "        \n",
    "        if link_id is not \"N/A\":\n",
    "            try: \n",
    "                submissions_dict[link_id]['comment_ids'].append(c_id)\n",
    "            except:\n",
    "                c_except +=1\n",
    "            \n",
    "print(\"Finished at \" + str(datetime.datetime.now()))\n",
    "print(\"Total time: --- {0} seconds ---\".format(round(time.time() - start_time, 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_d = json.dumps(comments_dict)\n",
    "f = open(\"../data/light_dict_json/comments_2018-03.json\",\"w\")\n",
    "f.write(json_d)\n",
    "f.close()\n",
    "\n",
    "json_d = json.dumps(submissions_dict)\n",
    "f = open(\"../data/light_dict_json/submissions_2018-03.json\",\"w\")\n",
    "f.write(json_d)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12022694"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(submissions_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lighten_submission_file(filename):\n",
    "    date_period = filename[-10:-3]\n",
    "    submissions_dict = dict()\n",
    "    with lzma.open(filename, \"r\") as f:\n",
    "        for i,line in enumerate(f):\n",
    "            json_obj= json.loads(line)\n",
    "            s_id = json_obj[\"id\"]\n",
    "\n",
    "            submissions_dict[s_id] = {\n",
    "                'author': json_obj['author'],\n",
    "                'subreddit': json_obj['subreddit'],\n",
    "                'subreddit_id': json_obj['subreddit_id'],\n",
    "                'created_utc': json_obj['created_utc'],\n",
    "                'title': json_obj['title'],\n",
    "                'comment_ids': [],\n",
    "                'score': json_obj['score'],\n",
    "                'selftext': json_obj['selftext']\n",
    "            }\n",
    "            \n",
    "    json_d = json.dumps(submissions_dict)\n",
    "    f = open(\"data/light_dict_json/\" + \"submissions_\" + date_period + \".json\",\"w\")\n",
    "    f.write(json_d)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lighten_comments_file(filename):\n",
    "    date_period = filename[-10:-3]\n",
    "    c_except=0\n",
    "    comments_dict = dict()\n",
    "\n",
    "    with lzma.open(filename, \"r\") as c_f:\n",
    "        for i,line in enumerate(c_f):\n",
    "            json_obj= json.loads(line)\n",
    "            c_id = json_obj[\"id\"]\n",
    "\n",
    "            link_id_full = json_obj[\"link_id\"]\n",
    "            if link_id_full is None:\n",
    "                link_id = \"N/A\"\n",
    "            else:\n",
    "                link_id = str(link_id_full)[3:]\n",
    "\n",
    "            parent_id_full = json_obj[\"parent_id\"]\n",
    "            if parent_id_full is None:\n",
    "                parent_id = \"N/A\"\n",
    "            else:\n",
    "                parent_id = str(parent_id_full)[3:]\n",
    "\n",
    "            comments_dict[c_id] = {\n",
    "                'author': json_obj['author'],\n",
    "                'subreddit': json_obj['subreddit'],\n",
    "                'subreddit_id': json_obj['subreddit_id'],\n",
    "                'created_utc': json_obj['created_utc'],\n",
    "                'parent_id': parent_id,\n",
    "                'link_id': link_id,\n",
    "                'score': json_obj['score'],\n",
    "                'body': json_obj['body']\n",
    "            }\n",
    "\n",
    "\n",
    "            if link_id is not \"N/A\":\n",
    "                try: \n",
    "                    submissions_dict[link_id]['comment_ids'].append(c_id)\n",
    "                except:\n",
    "                    c_except +=1\n",
    "                    \n",
    "    json_d = json.dumps(comments_dict)\n",
    "    f = open(\"data/light_dict_json/\" + \"comments_\" + date_period + \".json\",\"w\")\n",
    "    f.write(json_d)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Already done\n",
    "urls = {\n",
    "    'RS_2018-07.xz': 'https://files.pushshift.io/reddit/submissions/RS_2018-07.xz',\n",
    "    'RS_2018-06.xz': 'https://files.pushshift.io/reddit/submissions/RS_2018-06.xz',\n",
    "    'RS_2018-02.xz': 'https://files.pushshift.io/reddit/submissions/RS_2018-02.xz',\n",
    "    'RS_2018-01.xz': 'https://files.pushshift.io/reddit/submissions/RS_2018-01.xz',\n",
    "    'RC_2018-01.xz': 'https://files.pushshift.io/reddit/comments/RC_2018-01.xz',\n",
    "    'RC_2018-02.xz': 'https://files.pushshift.io/reddit/comments/RC_2018-02.xz',\n",
    "    'RC_2018-06.xz': 'https://files.pushshift.io/reddit/comments/RC_2018-06.xz',\n",
    "    'RC_2018-07.xz': 'https://files.pushshift.io/reddit/comments/RC_2018-07.xz'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = {\n",
    "    'RS_2018-03.xz': 'https://files.pushshift.io/reddit/submissions/RS_2018-03.xz',\n",
    "    'RS_2018-04.xz': 'https://files.pushshift.io/reddit/submissions/RS_2018-04.xz',\n",
    "    'RS_2018-05.xz': 'https://files.pushshift.io/reddit/submissions/RS_2018-05.xz',\n",
    "    'RS_2018-08.xz': 'https://files.pushshift.io/reddit/submissions/RS_2018-08.xz',\n",
    "    'RS_2018-09.xz': 'https://files.pushshift.io/reddit/submissions/RS_2018-09.xz',\n",
    "    'RS_2018-10.xz': 'https://files.pushshift.io/reddit/submissions/RS_2018-10.xz',\n",
    "    'RC_2018-03.xz': 'https://files.pushshift.io/reddit/comments/RC_2018-03.xz',\n",
    "    'RC_2018-04.xz': 'https://files.pushshift.io/reddit/comments/RC_2018-04.xz',\n",
    "    'RC_2018-05.xz': 'https://files.pushshift.io/reddit/comments/RC_2018-05.xz',\n",
    "    'RC_2018-08.xz': 'https://files.pushshift.io/reddit/comments/RC_2018-08.xz',\n",
    "    'RC_2018-09.xz': 'https://files.pushshift.io/reddit/comments/RC_2018-09.xz',\n",
    "    'RC_2018-10.xz': 'https://files.pushshift.io/reddit/comments/RC_2018-10.xz'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting at 2019-11-11 16:35:04.573818\n",
      "Finished at 2019-11-11 18:20:24.743809\n",
      "Total time: --- 6320.1714 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "print(\"Starting at \" + str(datetime.datetime.now()))\n",
    "\n",
    "for name, url in urls.items():\n",
    "    urllib.request.urlretrieve(url, \"data/\" + name)\n",
    "    \n",
    "print(\"Finished at \" + str(datetime.datetime.now()))\n",
    "print(\"Total time: --- {0} seconds ---\".format(round(time.time() - start_time, 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "urls.keys()\n",
    "urls_copy = copy.deepcopy(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting at 2019-11-11 18:20:24.808256\n",
      "Working on RS_2018-03.xz 2019-11-11 18:20:24.809027\n",
      "Working on RS_2018-04.xz 2019-11-11 18:42:10.385953\n",
      "Working on RS_2018-05.xz 2019-11-11 19:12:00.948597\n",
      "Working on RS_2018-08.xz 2019-11-11 19:41:07.439530\n",
      "Working on RS_2018-09.xz 2019-11-11 20:12:01.620723\n",
      "Working on RS_2018-10.xz 2019-11-11 20:44:48.482167\n",
      "Working on RC_2018-03.xz 2019-11-11 21:18:28.910793\n",
      "Working on RC_2018-04.xz 2019-11-11 23:23:00.910733\n",
      "Working on RC_2018-05.xz 2019-11-12 01:37:02.248035\n",
      "Working on RC_2018-08.xz 2019-11-12 03:58:36.957727\n",
      "Working on RC_2018-09.xz 2019-11-12 06:44:19.817406\n",
      "Working on RC_2018-10.xz 2019-11-12 09:29:18.177782\n",
      "Finished at 2019-11-12 12:27:25.375150\n",
      "Total time: --- 65220.5683 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "print(\"Starting at \" + str(datetime.datetime.now()))\n",
    "\n",
    "for filename in urls_copy.keys():\n",
    "    print(\"Working on \"+ filename + \" \" + str(datetime.datetime.now()))\n",
    "    \n",
    "    if filename.startswith('RS'):\n",
    "        lighten_submission_file(\"data/\" + str(filename))\n",
    "    elif filename.startswith('RC'):\n",
    "        lighten_comments_file(\"data/\" + str(filename))\n",
    "    else:\n",
    "        print(\"Filename \" + filename + \" not recognized!!\")\n",
    "    \n",
    "print(\"Finished at \" + str(datetime.datetime.now()))\n",
    "print(\"Total time: --- {0} seconds ---\".format(round(time.time() - start_time, 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### zstf files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = {\n",
    "    'RC_2018-10.zst': 'https://files.pushshift.io/reddit/comments/RC_2018-10.zst'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting at 2019-11-13 10:14:05.624091\n",
      "Finished at 2019-11-13 10:37:29.035604\n",
      "Total time: --- 1403.4127 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "print(\"Starting at \" + str(datetime.datetime.now()))\n",
    "\n",
    "for name, url in urls.items():\n",
    "    urllib.request.urlretrieve(url, \"data/\" + name)\n",
    "    \n",
    "print(\"Finished at \" + str(datetime.datetime.now()))\n",
    "print(\"Total time: --- {0} seconds ---\".format(round(time.time() - start_time, 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zstd\n",
    "import zstandard as zstd1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting at 2019-11-13 12:34:49.876651\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'zstandard' has no attribute 'ZstdCompressor'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-110-746677b23863>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mcctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzstd1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZstdCompressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream_reader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'zstandard' has no attribute 'ZstdCompressor'"
     ]
    }
   ],
   "source": [
    "urls_copy = copy.deepcopy(urls)\n",
    "\n",
    "start_time = time.time()\n",
    "print(\"Starting at \" + str(datetime.datetime.now()))\n",
    "\n",
    "with open(\"data/\" + str(filename), 'rb') as fh:\n",
    "    cctx = zstd1.ZstdCompressor()\n",
    "    reader = cctx.stream_reader(fh)\n",
    "    \n",
    "    while True:\n",
    "        chunk = reader.read(16384)\n",
    "        print(type(chunk))\n",
    "        break\n",
    "        if not chunk:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in urls_copy.keys():\n",
    "    print(\"Working on \"+ filename + \" \" + str(datetime.datetime.now()))\n",
    "    \n",
    "    data = zstd.decompress(filename)\n",
    "    \n",
    "    if filename.startswith('RS'):\n",
    "        lighten_submission_file_zst(data, \"data/\" + str(filename))\n",
    "    elif filename.startswith('RC'):\n",
    "        lighten_comments_file_zst(data, \"data/\" + str(filename))\n",
    "    else:\n",
    "        print(\"Filename \" + filename + \" not recognized!!\")\n",
    "    \n",
    "print(\"Finished at \" + str(datetime.datetime.now()))\n",
    "print(\"Total time: --- {0} seconds ---\".format(round(time.time() - start_time, 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lighten_submission_file_zst(data, filename):\n",
    "    date_period = filename[-10:-3]\n",
    "    submissions_dict = dict()\n",
    "\n",
    "    for i,line in enumerate(data):\n",
    "        json_obj= json.loads(line)\n",
    "        s_id = json_obj[\"id\"]\n",
    "\n",
    "        submissions_dict[s_id] = {\n",
    "            'author': json_obj['author'],\n",
    "            'subreddit': json_obj['subreddit'],\n",
    "            'subreddit_id': json_obj['subreddit_id'],\n",
    "            'created_utc': json_obj['created_utc'],\n",
    "            'title': json_obj['title'],\n",
    "            'comment_ids': [],\n",
    "            'score': json_obj['score'],\n",
    "            'selftext': json_obj['selftext']\n",
    "        }\n",
    "            \n",
    "    json_d = json.dumps(submissions_dict)\n",
    "    f = open(\"data/light_dict_json/\" + \"submissions_\" + date_period + \".json\",\"w\")\n",
    "    f.write(json_d)\n",
    "    f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
